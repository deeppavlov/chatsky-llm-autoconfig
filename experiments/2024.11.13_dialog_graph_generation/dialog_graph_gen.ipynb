{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cycle Graph Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Invalid transition from ['Great! Could you please provide your address?'] to ['Do you have any pets we should be aware of?'] via edge '[\"Here's my address\"]': The transition is invalid because the assistant's request for the user's address does not logically connect to the user's response of providing an address, followed by the assistant's question about pets. The assistant's question about pets seems unrelated to the address provided and does not follow a logical flow in the context of booking a house cleaning service.\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Invalid transition from ['Would you like any additional services, such as deep cleaning or window washing?'] to ['Thank you! Please provide your payment information to proceed.'] via edge '['Yes, I need additional services']': The transition is invalid because the user response does not specify which additional services they need, yet the assistant immediately asks for payment information. There is a missing step where the assistant should confirm the specific additional services before proceeding to payment.\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Invalid transition from ['What time of day would you prefer for your session?'] to ['Are you a returning client?'] via edge '['I prefer a session in the evening']': The transition is invalid because the assistant's response in the target utterance ('Are you a returning client?') does not logically follow from the user's preference for an evening session. The assistant should ideally respond with a follow-up question or confirmation related to the user's preferred time rather than asking about their client status.\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Invalid transition from ['Perfect! Could you please provide your contact information for enrollment?'] to ['Thank you for enrolling! Would you like to enroll in another course?'] via edge '['Sure, my email is example@example.com']': The transition is invalid because the source utterance asks for contact information for enrollment, while the edge utterance provides an email address. However, the target utterance jumps to thanking the user for enrolling without confirming that the contact information was successfully processed or that the enrollment was completed. There is a missing logical step in acknowledging the provided email and confirming the enrollment.\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph: graph_dict={'edges': [{'source': 1, 'target': 2, 'utterances': [\"I'd like to schedule a COVID-19 test\"]}, {'source': 2, 'target': 3, 'utterances': ['I prefer a drive-through test']}, {'source': 3, 'target': 4, 'utterances': ['Yes, that location works for me']}, {'source': 4, 'target': 5, 'utterances': [\"I'll be there at 10 AM\"]}, {'source': 5, 'target': 2, 'utterances': ['I need to schedule another test']}], 'nodes': [{'id': 1, 'label': 'welcome', 'is_start': True, 'utterances': ['Welcome! How can I assist you with COVID-19 testing today?']}, {'id': 2, 'label': 'ask_test_type', 'is_start': False, 'utterances': ['What type of COVID-19 test would you like to schedule?']}, {'id': 3, 'label': 'confirm_location', 'is_start': False, 'utterances': ['We have a drive-through test available at our main center. Does that work for you?']}, {'id': 4, 'label': 'confirm_time', 'is_start': False, 'utterances': ['Great! What time would you like to schedule your test?']}, {'id': 5, 'label': 'completed', 'is_start': False, 'utterances': ['Your appointment is confirmed. Would you like to schedule another test?']}]} graph=<networkx.classes.digraph.DiGraph object at 0x1273314c0> node_mapping={}\n",
      "Sampled dialogue: assistant: Welcome! How can I assist you with COVID-19 testing today?\n",
      "user: I'd like to schedule a COVID-19 test\n",
      "assistant: What type of COVID-19 test would you like to schedule?\n",
      "user: I prefer a drive-through test\n",
      "assistant: We have a drive-through test available at our main center. Does that work for you?\n",
      "user: Yes, that location works for me\n",
      "assistant: Great! What time would you like to schedule your test?\n",
      "user: I'll be there at 10 AM\n",
      "assistant: Your appointment is confirmed. Would you like to schedule another test?\n",
      "user: I need to schedule another test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Invalid transition from ['Thank you for your payment. Would you like a confirmation email?'] to ['Sure, what changes would you like to make to your subscription?'] via edge '['Yes, please send me the confirmation email']': The transition is invalid because the user is responding to a confirmation email request, but the assistant's follow-up question about changes to the subscription does not logically connect to the user's request for a confirmation email.\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:root:Invalid transition from ['Alright, let me know if the issue persists after trying the new solution.'] to [\"Can you describe the issue you're experiencing with your remote work setup?\"] via edge '[\"The new solution didn't work, my internet is still unstable\"]': The transition is invalid because the source utterance suggests that the assistant is providing a solution and waiting for feedback, while the edge utterance indicates that the user is still experiencing a problem. The target utterance then asks for a description of the issue, which is not a logical follow-up to the user's statement about the new solution not working. Instead, the assistant should acknowledge the user's continued issue before asking for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to: dataset/dialogue_dataset_1_topics_3.json\n"
     ]
    }
   ],
   "source": [
    "from chatsky_llm_autoconfig.algorithms.dialogue_augmentation import DialogAugmentation\n",
    "from chatsky_llm_autoconfig.algorithms.dialogue_generation import DialogueSampler\n",
    "from chatsky_llm_autoconfig.algorithms.topic_graph_generation import CycleGraphGenerator\n",
    "from chatsky_llm_autoconfig.metrics.llm_metrics import are_triplets_valid\n",
    "from typing import List, Dict, Any\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "def generate_dialogues(\n",
    "    topics: List[str], \n",
    "    num_augmentations: int = 2,\n",
    "    model_name: str = \"gpt-4o-mini\"\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Collects dialogues for each topic and returns them in a structured format.\n",
    "    \n",
    "    Args:\n",
    "        topics: List of conversation topics\n",
    "        num_augmentations: Number of augmented dialogues to generate per topic\n",
    "        model_name: Name of the model to use\n",
    "        \n",
    "    Returns:\n",
    "        List of dictionaries containing graph, topic, and dialogues for each topic\n",
    "    \"\"\"\n",
    "    # Initialize components\n",
    "    graph_generator = CycleGraphGenerator()\n",
    "    sampler = DialogueSampler()\n",
    "    augmenter = DialogAugmentation()\n",
    "    \n",
    "    model = ChatOpenAI(\n",
    "        model=model_name, \n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\"), \n",
    "        base_url=os.getenv(\"OPENAI_BASE_URL\"), \n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for topic in topics:\n",
    "        # Generate graph and validate\n",
    "        graph = graph_generator.invoke(topic=topic)\n",
    "        is_valid = are_triplets_valid(graph, model, topic)\n",
    "        \n",
    "        if is_valid[\"value\"]:\n",
    "            print(f\"Graph: {graph}\")\n",
    "            sampled_dialogue = sampler.invoke(graph, 1, -1)[0]\n",
    "            \n",
    "            print(f\"Sampled dialogue: {sampled_dialogue}\")\n",
    "            \n",
    "            # Initialize dialogues array with the sampled dialogue\n",
    "            dialogues = [sampled_dialogue.model_dump()]\n",
    "            \n",
    "            # Generate augmented versions\n",
    "            for _ in range(num_augmentations):\n",
    "                augmented = augmenter.invoke(dialogue=sampled_dialogue, topic=topic)\n",
    "                dialogues.append(augmented.model_dump())\n",
    "            \n",
    "            # Create result entry\n",
    "            result_entry = {\n",
    "                \"graph\": graph.graph_dict,\n",
    "                \"topic\": topic,\n",
    "                \"dialogues\": dialogues\n",
    "            }\n",
    "            \n",
    "            results.append(result_entry)\n",
    "            \n",
    "    return results\n",
    "\n",
    "def save_dataset(data: List[Dict[str, Any]], num_augmentations: int):\n",
    "    \"\"\"\n",
    "    Saves the dataset with a structured filename.\n",
    "    \n",
    "    Args:\n",
    "        data: List of dictionaries containing graph, topic, and dialogues\n",
    "        num_augmentations: Number of augmented dialogues generated\n",
    "    \"\"\"\n",
    "    # Create dataset directory if it doesn't exist\n",
    "    dataset_dir = Path(\"./dataset\")\n",
    "    dataset_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Generate filename with timestamp and dataset info\n",
    "    total_dialogues = 1 + num_augmentations\n",
    "    filename = f\"dialogue_dataset_{len(data)}_topics_{total_dialogues}.json\"\n",
    "    \n",
    "    # Full path for the file\n",
    "    file_path = dataset_dir / filename\n",
    "    \n",
    "    # Convert Graph objects to their dictionary representation\n",
    "    serializable_data = []\n",
    "    for entry in data:\n",
    "        serializable_entry = {\n",
    "            \"graph\": entry[\"graph\"],  # Use graph_dict instead of Graph object\n",
    "            \"topic\": entry[\"topic\"],\n",
    "            \"dialogues\": entry[\"dialogues\"]\n",
    "        }\n",
    "        serializable_data.append(serializable_entry)\n",
    "    \n",
    "    # Save the data\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(serializable_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"Dataset saved to: {file_path}\")\n",
    "    return file_path\n",
    "# Example usage\n",
    "topics = [\n",
    "    \"house cleaning service booking\",          # Home Services\n",
    "    \"online therapy session booking\",          # Mental Health\n",
    "    \"language course enrollment\",              # Education\n",
    "    \"covid-19 test scheduling\",               # Healthcare\n",
    "    \"food delivery subscription\",              # Food Service\n",
    "    \"remote work tech support\"                # Technical Support\n",
    "]\n",
    "\n",
    "# Number of augmentations (total dialogues will be num_augmentations + 1)\n",
    "num_augmentations = 2\n",
    "\n",
    "# Collect dialogues\n",
    "dialogue_collection = generate_dialogues(topics, num_augmentations=num_augmentations)\n",
    "\n",
    "saved_path = save_dataset(dialogue_collection, num_augmentations)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
