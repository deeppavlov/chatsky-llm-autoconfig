{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Graph Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "simple_graph_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Create a dialogue graph for a {topic} conversation that follows these rules:\n",
    "\n",
    "1. Each assistant message (node) must be a clear question or statement that expects a specific type of response\n",
    "2. Each user message (edge) must directly answer or respond to the previous assistant message\n",
    "3. Include these basic flows:\n",
    "   - Main success path (completing the primary task)\n",
    "   - Early exit path (user decides not to proceed)\n",
    "   - Return path (user wants to modify earlier choice)\n",
    "\n",
    "Example of correct flow:\n",
    "Assistant: \"What type of coffee would you like?\"\n",
    "User: \"A latte please\"\n",
    "Assistant: \"Would you like that hot or iced?\"\n",
    "User: \"Hot please\"\n",
    "\n",
    "Example of incorrect flow:\n",
    "Assistant: \"What type of coffee would you like?\"\n",
    "User: \"No thank you\" (This response doesn't match the question)\n",
    "\n",
    "Format:\n",
    "{{\n",
    "    \"edges\": [\n",
    "        {{ \"source\": 1, \"target\": 2, \"utterances\": [\"I'd like a coffee\"] }},\n",
    "        {{ \"source\": 2, \"target\": 3, \"utterances\": [\"A latte please\"] }}\n",
    "    ],\n",
    "    \"nodes\": [\n",
    "        {{ \"id\": 1, \"label\": \"welcome\", \"is_start\": true, \"utterances\": [\"Welcome! How can I help?\"] }},\n",
    "        {{ \"id\": 2, \"label\": \"ask_drink\", \"is_start\": false, \"utterances\": [\"What would you like to drink?\"] }}\n",
    "    ]\n",
    "}}\n",
    "\n",
    "Return ONLY the JSON without any additional text.\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chatsky_llm_autoconfig.algorithms.dialogue_generation import DialogueSampler\n",
    "from chatsky_llm_autoconfig.algorithms.topic_graph_generation import CycleGraphGenerator\n",
    "from chatsky_llm_autoconfig.metrics.llm_metrics import are_triplets_valid\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def generate_valid_dialogues(topics: list[str], output_dir: str = \"generated_datasets\") -> None:\n",
    "    # Setup\n",
    "    Path(output_dir).mkdir(exist_ok=True)\n",
    "    \n",
    "    # Initialize with default prompt if complex_graph_prompt is not defined\n",
    "    graph_generator = CycleGraphGenerator(prompt=simple_graph_prompt)  # Remove prompt parameter if not needed\n",
    "    sampler = DialogueSampler()\n",
    "    gen_model = ChatOpenAI(\n",
    "        model='o1-mini',\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        base_url=os.getenv(\"OPENAI_BASE_URL\"),\n",
    "    )\n",
    "    \n",
    "    valid_model = ChatOpenAI(\n",
    "        model='gpt-4o',\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        base_url=os.getenv(\"OPENAI_BASE_URL\"),\n",
    "    )\n",
    "    \n",
    "    valid_results = []\n",
    "    \n",
    "    # Generate and validate graphs\n",
    "    for topic in topics:\n",
    "        try:\n",
    "            # Generate and validate using the model name from your ChatOpenAI instance\n",
    "            graph = graph_generator.invoke(topic=topic, model=gen_model)\n",
    "            validation_result = are_triplets_valid(graph, valid_model)\n",
    "            \n",
    "            print(f\"Topic: {topic}\")\n",
    "            print(f\"Validation result: {validation_result}\")\n",
    "            \n",
    "            # If valid, sample dialogue and save\n",
    "            if validation_result[\"value\"]:\n",
    "                sampled_dialogues = sampler.invoke(graph, 1, -1)\n",
    "                valid_results.append({\n",
    "                    \"graph\": graph.graph_dict,\n",
    "                    \"topic\": topic,\n",
    "                    \"dialogues\": [d.model_dump() for d in sampled_dialogues],\n",
    "                    \"validation_result\": validation_result\n",
    "                })\n",
    "                print(f\"✅ Valid dialogue generated for topic: {topic}\")\n",
    "            else:\n",
    "                print(f\"❌ Invalid dialogue for topic: {topic}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {topic}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    # Save results\n",
    "    if valid_results:\n",
    "        output_file = Path(output_dir) / f\"valid_dialogues_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(valid_results, f, ensure_ascii=False, indent=2)\n",
    "        print(f\"\\nSaved {len(valid_results)} valid dialogues to: {output_file}\")\n",
    "\n",
    "\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run generation\n",
    "topics = [\n",
    "    \"medical appointment scheduling\",\n",
    "    \"food delivery service\",\n",
    "    \"fitness membership registration\",\n",
    "    \"apartment rental application\",\n",
    "    \"tech support assistance\",\n",
    "    \"travel package booking\",\n",
    "    \"insurance policy purchase\",\n",
    "    \"pet grooming service\",\n",
    "    \"moving service arrangement\",\n",
    "    \"home cleaning service\"\n",
    "]\n",
    "\n",
    "\n",
    "# generate_valid_dialogues(topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chatsky_llm_autoconfig.algorithms.dialogue_generation import DialogueSampler\n",
    "from chatsky_llm_autoconfig.graph import Graph\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Initialize sampler\n",
    "sampler = DialogueSampler()\n",
    "\n",
    "# Get all JSON files in the directory\n",
    "dataset_dir = Path(\"generated_datasets\")\n",
    "json_files = list(dataset_dir.glob(\"*.json\"))\n",
    "\n",
    "for json_path in json_files:\n",
    "    print(f\"\\nProcessing file: {json_path.name}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Load JSON file\n",
    "    with open(json_path, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Process each graph\n",
    "    for entry in data:\n",
    "        print(f\"\\nTopic: {entry['topic']}\")\n",
    "        graph = Graph(entry[\"graph\"])\n",
    "        \n",
    "        # Sample dialogues\n",
    "        dialogues = sampler.invoke(graph, 1, -1)\n",
    "        \n",
    "        # Print all dialogues\n",
    "        for i, dialogue in enumerate(dialogues, 1):\n",
    "            print(f\"\\nDialogue {i}:\")\n",
    "            for message in dialogue.messages:\n",
    "                print(f\"- {message}\")\n",
    "            \n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing graphs from: graph_gen/graph_gen.json\n",
      "\n",
      "Validating graph with topic: library\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:openai._base_client:Retrying request to /chat/completions in 0.386954 seconds\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST http://193.187.173.33:8002/api/providers/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation result: {'value': True, 'description': 'All transitions are valid.'}\n",
      "✅ Valid dialogue generated for topic: library\n",
      "\n",
      "Saved 1 valid dialogues to: valid_complex_graphs/valid_dialogues_20241217_172116.json\n"
     ]
    }
   ],
   "source": [
    "from chatsky_llm_autoconfig.algorithms.dialogue_generation import DialogueSampler\n",
    "from chatsky_llm_autoconfig.graph import BaseGraph\n",
    "from chatsky_llm_autoconfig.metrics.llm_metrics import are_triplets_valid\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def check_existing_graphs_and_sample() -> None:\n",
    "   sampler = DialogueSampler()\n",
    "   \n",
    "   valid_model = ChatOpenAI(\n",
    "       model='gpt-4o',\n",
    "       api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "       base_url=os.getenv(\"OPENAI_BASE_URL\"),\n",
    "   )\n",
    "   \n",
    "   # Path to the directory containing generated datasets\n",
    "   datasets_dir = Path(\"graph_gen\")\n",
    "   \n",
    "   valid_results = []\n",
    "   \n",
    "   # Iterate through json files in the datasets directory\n",
    "   for file_path in datasets_dir.glob(\"*.json\"):\n",
    "       try:\n",
    "           # Load existing graph array\n",
    "           with open(file_path, 'r', encoding='utf-8') as f:\n",
    "               graphs_data = json.load(f)\n",
    "           \n",
    "           print(f\"\\nProcessing graphs from: {file_path}\")\n",
    "           \n",
    "           # Process each graph in the array\n",
    "           for graph_data in graphs_data:\n",
    "               print(f\"\\nValidating graph with topic: {graph_data['topic']}\")\n",
    "               \n",
    "               graph_obj: BaseGraph = Graph(graph_data['graph'])\n",
    "\n",
    "               # Validate triplets\n",
    "               validation_result = are_triplets_valid(graph_obj, valid_model)\n",
    "               print(f\"Validation result: {validation_result}\")\n",
    "               \n",
    "               \n",
    "               if validation_result[\"value\"]:\n",
    "                   sampled_dialogues = sampler.invoke(graph_obj, 1, -1)\n",
    "                   valid_results.append({\n",
    "                       \"graph\": graph_data['graph'],\n",
    "                       \"topic\": graph_data['topic'],\n",
    "                       \"dialogues\": [d.model_dump() for d in sampled_dialogues],\n",
    "                       \"validation_result\": validation_result\n",
    "                   })\n",
    "                   print(f\"✅ Valid dialogue generated for topic: {graph_data['topic']}\")\n",
    "               else:\n",
    "                   print(f\"❌ Invalid dialogue for topic: {graph_data['topic']}\")\n",
    "               \n",
    "       except Exception as e:\n",
    "           print(f\"Error processing {file_path}: {str(e)}\")\n",
    "           continue\n",
    "\n",
    "   # Save results\n",
    "   if valid_results:\n",
    "       output_dir = Path(\"valid_complex_graphs\")\n",
    "       output_dir.mkdir(exist_ok=True)\n",
    "       \n",
    "       output_file = output_dir / f\"valid_dialogues_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "       with open(output_file, 'w', encoding='utf-8') as f:\n",
    "           json.dump(valid_results, f, ensure_ascii=False, indent=2)\n",
    "       print(f\"\\nSaved {len(valid_results)} valid dialogues to: {output_file}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   check_existing_graphs_and_sample()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
